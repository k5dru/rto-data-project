{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e99444",
   "metadata": {},
   "source": [
    "# goal:  fetch the minimal set of data from SPP sources for the current interval or a past interval\n",
    "# persist in a database (try postgresql)\n",
    "\n",
    "\n",
    " ## TODO\n",
    " - DONE: figure out if we are ON or OFF the renaming of columns bandwagon.  We are ON the rename bandwagon for now, but a change to standardize_columns would change that.\n",
    "     Background: https://dba.stackexchange.com/questions/250943/should-i-not-use-camelcase-in-my-column-names\n",
    " \n",
    " - REFACTOR to get the most recent available RTBM file, and use that as a time basis. \n",
    "     * Combine as much as possible in a common function \n",
    "     * Backfill: Provide the ability to query recent intervals that are not in the database, create a valid path from interval timestamps (stlf appears tricky), and slowly backfill a day of data\n",
    "     * Optional Forward fill: for Multi-Day Resource Assessment, provide a way to forward-fill for a few days\n",
    " - CLEAN UP so that when exported to a python file it just runs without edits, and produces sane reports from each module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c00ab",
   "metadata": {},
   "source": [
    "### Prerequisite:  \n",
    "* miniconda installation from 2023 or later \n",
    "* in anaconda powershell prompt, \"conda activate 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8106e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install duckdb\n",
    "import pandas as pd \n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc299e87",
   "metadata": {},
   "source": [
    "Source data model is in https://docs.google.com/spreadsheets/d/1Qh28Lb4dcbw9YMqcXLSj7N8l6Tlr46xNQkV-t1A2txc/edit#gid=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c814d8",
   "metadata": {},
   "source": [
    "## Define local database. Duckdb is cool and all, but how about a real database? \n",
    "\n",
    "  \n",
    "    \n",
    "    to initiate session:\n",
    "    SET default_tablespace = u02_pgdata;\n",
    "    create schema if not exists sppdata authorization current_user;\n",
    "    set search_path to sppdata;\n",
    "    \n",
    "    to drop everything: \n",
    "     select 'drop table ' || table_name || ' cascade;' from information_schema.tables where table_schema = 'sppdata';\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c947470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# read the database information from the json file\n",
    "with open('../dbconn.json', 'r') as f:\n",
    "    di = json.load(f)\n",
    "# create a connection string for postgresql\n",
    "pg_uri = f\"//{di['username']}:{di['password']}@{di['host']}:{di['port']}/{di['database']}\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac2b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sqlalchemy\n",
    "#!pip install psycopg2\n",
    "\n",
    "# using https://pythontic.com/pandas/serialization/postgresql as example\n",
    "# though https://naysan.ca/2020/05/31/postgresql-to-pandas/ avoids the sqlalchemy layer\n",
    "# Example python program to read data from a PostgreSQL table\n",
    "# and load into a pandas DataFrame\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine   = create_engine(f'postgresql+psycopg2:{pg_uri}', pool_recycle=3600);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209110c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Connect to PostgreSQL server\n",
    "con    = alchemyEngine.connect();\n",
    "# con.execute (text(\"SET default_tablespace = u02_pgdata\"))\n",
    "# con.execute (text(\"create schema if not exists sppdata authorization current_user\"))\n",
    "# con.execute (text(\"set search_path to sppdata\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ef8419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con.autocommit=False;\n",
    "# Read data from PostgreSQL database table and load into a DataFrame instance\n",
    "#dataFrame       = pd.read_sql(text(\"select * from information_schema.tables\"), con);\n",
    "#dataFrame\n",
    " \n",
    "def pgsqldf(query): \n",
    "    return pd.read_sql(text(query), con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3643cc8",
   "metadata": {},
   "source": [
    "### example dataframe to table \n",
    "\n",
    "    dataFrame.to_sql(\"test_table\", con=con, if_exists='replace', index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9d0f6",
   "metadata": {},
   "source": [
    "### example table to dataframe: \n",
    "    df=pd.read_sql(text(\"select * from information_schema.tables\"), con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit();  #  OH YEAH!  I'm on a real database!  changes need to be committed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if wanting to nuke the world and start over: \n",
    "if False: \n",
    "    for table_name in ['rtbm_lmp_by_location', \n",
    "        'da_lmp_by_location', \n",
    "        'area_control_error', \n",
    "        'stlf_vs_actual', \n",
    "        'mtlf_vs_actual', \n",
    "        'tie_flows_long', \n",
    "        'rtbm_binding_constraints',\n",
    "        'generation_mix',\n",
    "        'settlement_location',\n",
    "    ]: \n",
    "        con.execute(text(f\"drop table {table_name} cascade\"))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e302cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to transform source data column names to a more appropriate form for working with data:\n",
    "def standardize_columns(df): \n",
    "    df.columns = (df.columns\n",
    "                    .str.replace('^ ', '', regex=True)\n",
    "                    .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                    .str.replace('[_ ]+', '_', regex=True)\n",
    "                    .str.lower()\n",
    "                 )    \n",
    "    \n",
    "    # add an inserted time to all dataframes\n",
    "    if not 'inserted_time' in df.columns.values: \n",
    "        df['inserted_time'] = datetime.now(pytz.timezone(\"America/Chicago\"))\n",
    "\n",
    "     \n",
    "    print(\"standardized columns: \",  df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be0ee5",
   "metadata": {},
   "source": [
    "# Settlement Locations\n",
    "and their estimated locations, from a local project file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    # todo:  store the notebook that created this file and this file in documentation somewhere\n",
    "    df=pd.read_csv(\"c:/Users/Operator/Downloads/UALR_MastersProject/SPPLocation/settlement_node_location.csv\")\n",
    "\n",
    "    \"\"\"\":# for database operations, rename all table and field names to lowercase snake_case\n",
    "    df.rename(columns={'Settlement Location':'settlement_location',\n",
    "               'InferredLocationType':'inferred_location_type',\n",
    "               'est_Latitude':'est_latitude',\n",
    "               'est_Longitude':'est_longitude'}, inplace=True)\n",
    "    \"\"\"\n",
    "\n",
    "    standardize_columns(df)\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45286753",
   "metadata": {},
   "source": [
    "    # per https://duckdb.org/docs/guides/python/import_pandas.html, duckdb just knows about dataframes; no import necessary\n",
    "    # create the table \"my_table\" from the DataFrame \"my_df\"\n",
    "    duckdb.sql(\"CREATE or replace TABLE settlement_location AS SELECT * FROM df\")\n",
    "    # or\n",
    "    # insert into the table \"my_table\" from the DataFrame \"my_df\"\n",
    "    # duckdb.sql(\"INSERT INTO my_table SELECT * FROM my_df\")\n",
    "\n",
    "    # check that it got there\n",
    "    duckdb.sql(\"SELECT table_catalog, table_name, column_name, is_nullable, data_type from information_schema.columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c18a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    # this only needs to be done when settlement locations table changes, and now views depend on it.\n",
    "\n",
    "    df.to_sql(\"settlement_location\", con=con, if_exists='replace', index=False); \n",
    "\n",
    "    con.execute(text(\"\"\"alter table settlement_location \n",
    "    add constraint settlement_location_pk \n",
    "    primary key (settlement_location)\"\"\"\n",
    "                    )\n",
    "               )\n",
    "\n",
    "    con.commit()  #  OH YEAH!  I'm on a real database!  changes need to be committed.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a986a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select sample data for fun\n",
    "pd.read_sql(text(\"SELECT * from settlement_location order by random() limit 10\"), con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe72365",
   "metadata": {},
   "source": [
    "### todo: Bug: if first append works but primary key fails to create, nothing else will work ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_insertnew(table_name, primary_keys, df, con):\n",
    "    # insert df into table_name but only if those rows aren't already there\n",
    "    \n",
    "    # make sure the target table exists, but empty (by iloc[0:0])\n",
    "    try: \n",
    "        print (\"pg_insertnew: trying append\", table_name, con)\n",
    "        con.commit() # maybe it needs no outstanding transactions before a DDL?  I think that was it\n",
    "        df.to_sql(table_name, con=con, if_exists='append', index=False); \n",
    "        try_merge=False\n",
    "    except: \n",
    "        print (\"pg_insertnew append failed, rolling back\")\n",
    "        con.rollback();\n",
    "        try_merge=True\n",
    "\n",
    "        print (\"pg_insertnew making sure table has PK\")\n",
    "\n",
    "    # make sure the target table has a primary key(s)\n",
    "    con.execute(text(f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "        if NOT exists (\n",
    "          select constraint_name\n",
    "          from information_schema.table_constraints\n",
    "          where table_name = '{table_name}' \n",
    "          and constraint_type = 'PRIMARY KEY'\n",
    "        ) then ALTER TABLE {table_name}\n",
    "          ADD CONSTRAINT {table_name}_pk PRIMARY KEY ({','.join(primary_keys)});\n",
    "        end if;\n",
    "    end $$\"\"\"))\n",
    "    \n",
    "    con.commit();\n",
    "\n",
    "    if try_merge: \n",
    "#        print (\"trying merge\")\n",
    "        print (f\"pg_insertnew loading {table_name}_stg\")\n",
    "        # load df to a stage table\n",
    "        df.to_sql(f\"{table_name}_stg\", con=con, if_exists='replace', index=False); \n",
    "        con.commit();\n",
    "\n",
    "        # Insert new rows into permanent table\n",
    "        joinstring=''\n",
    "        for k in primary_keys: \n",
    "            if k == primary_keys[0]: \n",
    "                joinstring += f's.{k} = g.{k}'\n",
    "            else:\n",
    "                joinstring += f' and s.{k} = g.{k}'\n",
    "\n",
    "        print (f\"pg_insertnew loading {table_name}_stg to {table_name}\")\n",
    "        con.execute(text(f\"\"\"\n",
    "           insert into {table_name}\n",
    "           select s.* from {table_name}_stg s\n",
    "           left join {table_name} g\n",
    "           on {joinstring}\n",
    "           where g.{primary_keys[0]} IS NULL\n",
    "        \"\"\"))\n",
    "\n",
    "        con.commit();\n",
    "        con.execute (text(f\"\"\"drop table {table_name}_stg\"\"\"))\n",
    "        con.commit();    \n",
    "                     \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b91579",
   "metadata": {},
   "source": [
    "# Generation Mix\n",
    "\n",
    "## todo: handle web server errors like \n",
    "- IncompleteRead: IncompleteRead(4044 bytes read, 2 more expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try something harder: 2 hour generation mix. \n",
    "\n",
    "def update_generation_mix(con):\n",
    "    df=pd.read_csv(\"https://marketplace.spp.org/file-browser-api/download/generation-mix-historical?path=%2FGenMix2Hour.csv\", \n",
    "                   parse_dates=['GMT MKT Interval'], \n",
    "                   infer_datetime_format = True)\n",
    "   \n",
    "    standardize_columns(df)\n",
    "\n",
    "    pg_insertnew(table_name='generation_mix', primary_keys=['gmt_mkt_interval'], df=df, con=con)\n",
    "    \n",
    "    return pgsqldf(\"select * from generation_mix order by gmt_mkt_interval desc limit 5\")\n",
    "\n",
    "# update_generation_mix(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8895ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test query, just because I can:\n",
    "\n",
    "pgsqldf(\"\"\"\n",
    "    select gmt_mkt_interval, \n",
    "     gmt_mkt_interval at time zone 'America/Chicago' as local_interval,\n",
    "    coal_market+coal_self as coal,\n",
    "    diesel_fuel_oil_market+diesel_fuel_oil_self as diesel,\n",
    "    hydro_market+hydro_self as hydro,\n",
    "    natural_gas_market+natural_gas_self as natural_gas,\n",
    "    nuclear_market+nuclear_self as nuclear,\n",
    "    solar_market+solar_self as solar,\n",
    "    wind_market+wind_self as wind,\n",
    "    waste_disposal_services_market+waste_disposal_services_self\n",
    "      +waste_heat_market+waste_heat_self\n",
    "      +other_market+other_self as other,\n",
    "    load \n",
    "    from generation_mix\n",
    "    order by gmt_mkt_interval desc\n",
    "    limit 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdfcc39",
   "metadata": {},
   "source": [
    "# RTBM LMP by Settlement Location\n",
    "\n",
    "Depends on:  \n",
    "generation_mix table, for the most recent interval. \n",
    "Will be stored in the ci (current interval) dataframe.\n",
    "\n",
    "### TODO\n",
    " * determine if file names change with DST, and what the duplicate hour in November looks like\n",
    " * switch to grabbing that \"latest interval\" file\n",
    "     * on FTP at pubftp.spp.org/Markets/RTBM/LMP_By_SETTLEMENT_LOC/RTBM-LMP-SL-latestInterval.csv \n",
    "     * on HTTPS at https://marketplace.spp.org/file-browser-api/download/rtbm-lmp-by-location?path=%2FRTBM-LMP-SL-latestInterval.csv\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44502070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_interval(): \n",
    "    retdf = pgsqldf(\"\"\"\n",
    "    with c as (\n",
    "        select max(gmt_mkt_interval) at time zone 'America/Chicago' as interval_cpt\n",
    "        from generation_mix\n",
    "    )\n",
    "    , intervalmunge as (\n",
    "        select interval_cpt, \n",
    "        '1970-01-01 00:00:00'::timestamp + (interval '5 minutes' * (floor(extract(EPOCH from interval_cpt)::numeric / 300.0) + 1)) as interval_end_cpt,\n",
    "        '1970-01-01 00:00:00'::timestamp + (interval '1 hour' * (floor(extract(EPOCH from interval_cpt)::numeric / 3600.0) + 1)) as hour_end_cpt,\n",
    "        '1970-01-01 00:00:00'::timestamp + (interval '1 hour' * (floor(extract(EPOCH from (interval_cpt + interval '5 minutes'))::numeric / 3600.0) + 1)) as pathhour_end_cpt  \n",
    "        from c\n",
    "    )\n",
    "    select \n",
    "/*     date_part('year', interval_end_cpt)::char as rt_yyyy,\n",
    "    lpad(date_part('month', interval_end_cpt)::char, 2, '0') as rt_mm,\n",
    "    lpad(date_part('day', interval_end_cpt)::char, 2, '0') as rt_dd,\n",
    "    lpad(date_part('hour', interval_end_cpt)::char, 2, '0') as rt_hh24,\n",
    "    lpad(date_part('minute', interval_end_cpt)::char, 2, '0') as rt_mi,\n",
    "*/\n",
    "    to_char(interval_end_cpt, 'YYYY') as rt_yyyy,\n",
    "    to_char(interval_end_cpt, 'MM') as rt_mm,\n",
    "    to_char(interval_end_cpt, 'DD') as rt_dd,\n",
    "    to_char(interval_end_cpt, 'HH24') as rt_hh24,\n",
    "    to_char(interval_end_cpt, 'MI') as rt_mi,\n",
    "    \n",
    "    to_char(hour_end_cpt, 'YYYY') as da_yyyy,\n",
    "    to_char(hour_end_cpt, 'MM') as da_mm,\n",
    "    to_char(hour_end_cpt, 'DD') as da_dd,\n",
    "    to_char(hour_end_cpt, 'HH24') as da_hh24,\n",
    "    to_char(pathhour_end_cpt, 'HH24') as pathda_hh24\n",
    "    \n",
    "    from intervalmunge\n",
    "    \"\"\")\n",
    "    return retdf\n",
    "\n",
    "con.rollback()\n",
    "get_current_interval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rtbm_lmp(con):\n",
    "    # Pull out of generation_mix the most recent interval, in a format needed to get other information: \n",
    "    ci = get_current_interval()\n",
    "    \n",
    "    rt_yyyy=ci.rt_yyyy.values[0]\n",
    "    rt_mm  =ci.rt_mm.values[0]\n",
    "    rt_dd  =ci.rt_dd.values[0]\n",
    "    rt_hh24  =ci.rt_hh24.values[0]\n",
    "    rt_mi  =ci.rt_mi.values[0]\n",
    "\n",
    "    # if this interval exists already in rtbm_lmp_by_location, skip the rest\n",
    "    try: \n",
    "        rtbm_db_df=pgsqldf(f\"\"\"\n",
    "        select *\n",
    "        from rtbm_lmp_by_location\n",
    "        where (gmtinterval_end at time zone 'America/Chicago') = '{rt_yyyy}-{rt_mm}-{rt_dd} {rt_hh24}:{rt_mi}:00'\n",
    "        limit 5\n",
    "        \"\"\")\n",
    "\n",
    "        assert len(rtbm_db_df.index) > 0\n",
    "\n",
    "        return rtbm_db_df\n",
    "            \n",
    "    except: \n",
    "        get_rtbm=True\n",
    "\n",
    "    print (\"updating RTBM data from current interval:\", get_rtbm)\n",
    "\n",
    "    fpath=f\"https://marketplace.spp.org/file-browser-api/download/rtbm-lmp-by-location?\" + \\\n",
    "          f\"path=%2F{rt_yyyy}%2F{rt_mm}%2FBy_Interval%2F{rt_dd}%2F\" + \\\n",
    "          f\"RTBM-LMP-SL-{rt_yyyy}{rt_mm}{rt_dd}{rt_hh24}{rt_mi}.csv\"\n",
    "\n",
    "# todo:  handle 404 gracefully.  Sometimes the RTBM doesn't solve. \n",
    "\n",
    "    print (f\"reading {fpath}\")\n",
    "\n",
    "    dfnew=pd.read_csv(fpath, parse_dates=['GMTIntervalEnd'], \n",
    "                   infer_datetime_format = True)\n",
    "\n",
    "    \"\"\"\n",
    "    dfnew.rename(columns={'Interval':'interval', 'GMTIntervalEnd':'gmt_interval_end', 'Settlement Location':'settlement_location',\n",
    "                   'Pnode':'pnode', 'LMP':'lmp', 'MLC':'mlc', 'MCC':'mcc', 'MEC':'mec'}, inplace=True)\n",
    "    \"\"\" \n",
    "#   in this source file, GMTIntervalEnd does not have a timezone.  Add it: \n",
    "    from datetime import timezone\n",
    "    dfnew['GMTIntervalEnd'] = dfnew['GMTIntervalEnd'].dt.tz_localize(timezone.utc)\n",
    "\n",
    "    standardize_columns(dfnew)\n",
    "    \n",
    "# interval is now redundant \n",
    "    dfnew.drop(axis='columns', columns=['interval'], inplace=True)\n",
    "\n",
    "#    return dfnew\n",
    "\n",
    "    # insert rows that don't already exist\n",
    "    \n",
    "    pg_insertnew('rtbm_lmp_by_location', ['gmtinterval_end', 'settlement_location'], dfnew, con)\n",
    "        \n",
    "    con.commit()    \n",
    "    \n",
    "    rtbm_db_df=pgsqldf(f\"\"\"\n",
    "       SELECT * \n",
    "       from rtbm_lmp_by_location \n",
    "       order by gmtinterval_end desc, random() limit 5\n",
    "    \"\"\")\n",
    "    return rtbm_db_df   \n",
    "\n",
    "#con.rollback()\n",
    "#update_rtbm_lmp(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47034aee",
   "metadata": {},
   "source": [
    "# DA LMP by Settlement Location\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd612e",
   "metadata": {},
   "source": [
    "# TODO: fix the test in RTBM and DALMP that used 'interval' to determine if it needed to reload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_da_lmp(con):\n",
    "    # Pull out of generation_mix the most recent interval, in a format needed to get other information: \n",
    "    ci = get_current_interval()\n",
    "    \n",
    "    da_yyyy=ci.da_yyyy.values[0]\n",
    "    da_mm  =ci.da_mm.values[0]\n",
    "    da_dd  =ci.da_dd.values[0]\n",
    "    da_hh24  =ci.da_hh24.values[0]\n",
    "\n",
    "        # if this interval exists already in da_lmp_by_location, skip the rest\n",
    "    try: \n",
    "        da_db_df=pgsqldf(f\"\"\"\n",
    "        select *\n",
    "        from da_lmp_by_location\n",
    "        where (gmtinterval_end at time zone 'America/Chicago') = '{da_yyyy}-{da_mm}-{da_dd} {da_hh24}:00:00'\n",
    "        limit 5\n",
    "        \"\"\")\n",
    "\n",
    "        assert len(da_db_df.index) > 0\n",
    "\n",
    "        return da_db_df\n",
    "    except: \n",
    "        get_da=True\n",
    "\n",
    "    print (\"updating DA data from current interval:\", get_da)\n",
    "\n",
    "    fpath=f\"https://marketplace.spp.org/file-browser-api/download/da-lmp-by-location?\" + \\\n",
    "          f\"path=%2F{da_yyyy}%2F{da_mm}%2FBy_Day%2FDA-LMP-SL-{da_yyyy}{da_mm}{da_dd}0100.csv\"\n",
    "        \n",
    "    print (f\"reading {fpath}\")\n",
    "\n",
    "    try: \n",
    "        # these are big; if I've already run once today it is cached\n",
    "        dfnew=pd.read_pickle(f\"DA-LMP-SL-{da_yyyy}{da_mm}{da_dd}0100.pickle\")\n",
    "        print (f\"read local cached version DA-LMP-SL-{da_yyyy}{da_mm}{da_dd}0100.pickle\")\n",
    "    except: \n",
    "        dfnew=pd.read_csv(fpath, parse_dates=['GMTIntervalEnd'], \n",
    "                   infer_datetime_format = True)\n",
    "        dfnew.to_pickle(f\"DA-LMP-SL-{da_yyyy}{da_mm}{da_dd}0100.pickle\")\n",
    "        print (f\"saved local cached version DA-LMP-SL-{da_yyyy}{da_mm}{da_dd}0100.pickle\")\n",
    "    \"\"\"\n",
    "    dfnew.rename(columns={'Interval':'interval', 'GMTIntervalEnd':'gmt_interval_end', 'Settlement Location':'settlement_location',\n",
    "                   'Pnode':'pnode', 'LMP':'lmp', 'MLC':'mlc', 'MCC':'mcc', 'MEC':'mec'}, inplace=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    #   in this source file, GMTIntervalEnd does not have a timezone.  Add it: \n",
    "    from datetime import timezone\n",
    "    dfnew['GMTIntervalEnd'] = dfnew['GMTIntervalEnd'].dt.tz_localize(timezone.utc)\n",
    "\n",
    "    standardize_columns(dfnew)\n",
    "# interval is now redundant \n",
    "    dfnew.drop(axis='columns', columns=['interval'], inplace=True)\n",
    "\n",
    "    \n",
    "    # insert rows that don't already exist\n",
    "    pg_insertnew('da_lmp_by_location', ['gmtinterval_end', 'settlement_location'], dfnew, con)\n",
    "        \n",
    "    con.commit()    \n",
    "    \n",
    "    da_db_df=pgsqldf(f\"\"\"\n",
    "       SELECT * \n",
    "       from da_lmp_by_location \n",
    "       order by gmtinterval_end desc, random() limit 5\n",
    "       \"\"\")\n",
    "    return da_db_df   \n",
    "\n",
    "#con.rollback()\n",
    "#update_da_lmp(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9fbd8",
   "metadata": {},
   "source": [
    "# Area Control Error\n",
    "    ftp://pubftp.spp.org/Operational_Data/ACE/ACE.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c52a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ace(con):\n",
    "    table_name=\"area_control_error\"\n",
    "    source_url=\"ftp://pubftp.spp.org/Operational_Data/ACE/ACE.csv\"\n",
    "    primary_keys=['gmttime']\n",
    "    \n",
    "    df=pd.read_csv(source_url, \n",
    "                   parse_dates=['GMTTime'], \n",
    "                   infer_datetime_format = True\n",
    "                  )\n",
    "    \n",
    "    print (df.columns.values)\n",
    "    \n",
    "    # df.rename(columns={'GMTTime':'gmt_time', 'Value':'value'}, inplace=True)\n",
    "    standardize_columns(df)  \n",
    "    print(df.columns)\n",
    "    \n",
    "    pg_insertnew(table_name=table_name, primary_keys=primary_keys, df=df, con=con)\n",
    "    \n",
    "    con.commit()\n",
    "    \n",
    "    return pgsqldf(f\"select * from {table_name} order by gmttime desc limit 5\")\n",
    "\n",
    "#update_ace(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e819b",
   "metadata": {},
   "source": [
    "# STLF vs. Actual\n",
    "    https://marketplace.spp.org/file-browser-api/download/stlf-vs-actual?path=%2F2023%2F02%2F25%2F15%2FOP-STLF-202302251435.csv\n",
    "    \n",
    "## TODO:  this one is strange; is there a file with more data?  If not, \n",
    "## need to figure out how replace older rows with newer ones while keeping ones that have already aged out\n",
    "\n",
    "DONE - maybe delete rows with NULLs in Actual column before inserting new values?  If done in one transaction \n",
    "    * already done in Tie Flows; just do that\n",
    "- need to remove commits from the pg_insertnew to keep client from seeing missing data between delete and insert\n",
    "\n",
    "- at 23:30, tried to read\n",
    "https://marketplace.spp.org/file-browser-api/download/stlf-vs-actual?path=/2023/03/01/23/OP-STLF-202303012330.csv\n",
    "- but it is not there; latest file is pubftp.spp.org/Operational_Data/STLF/2023/03/01/00/OP-STLF-202303012330.csv\n",
    "\n",
    "## fix start-of-hour 404: \n",
    "## at about 11:03, this was 404: \n",
    "    #  https://marketplace.spp.org/file-browser-api/download/stlf-vs-actual?path=/2023/03/02/11/OP-STLF-202303021100.csv\n",
    "    # found here\n",
    "    #  https://marketplace.spp.org/file-browser-api/download/stlf-vs-actual?path=/2023/03/02/12/OP-STLF-202303021100.csv\n",
    "    # so the value of da_hh24 in the path had already advanced, 5 minutes early\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stlf(con):\n",
    "    \n",
    "    # Pull out of generation_mix the most recent interval, in a format needed to get other information: \n",
    "    ci = get_current_interval()\n",
    "    \n",
    "    rt_yyyy=ci.rt_yyyy.values[0]\n",
    "    da_yyyy=ci.da_yyyy.values[0]\n",
    "    rt_mm  =ci.rt_mm.values[0]\n",
    "    da_mm  =ci.da_mm.values[0]\n",
    "    rt_dd  =ci.rt_dd.values[0]\n",
    "    da_dd  =ci.da_dd.values[0]\n",
    "    rt_hh24  =ci.rt_hh24.values[0]\n",
    "    da_hh24  =ci.da_hh24.values[0]\n",
    "    pathda_hh24  =ci.pathda_hh24.values[0]\n",
    "    rt_mi  =ci.rt_mi.values[0]\n",
    "    \n",
    "    table_name=\"stlf_vs_actual\"\n",
    "    \n",
    "    source_url=f\"https://marketplace.spp.org/file-browser-api/download/stlf-vs-actual?\" + \\\n",
    "               f\"path=%2F{rt_yyyy}%2F{rt_mm}%2F{rt_dd}%2F{pathda_hh24}%2FOP-STLF-{rt_yyyy}{rt_mm}{rt_dd}{rt_hh24}{rt_mi}.csv\"\n",
    "    primary_keys=['gmtinterval_end']\n",
    "    \n",
    "    print (\"reading\", source_url)\n",
    "    \n",
    "    df=pd.read_csv(source_url, \n",
    "                   parse_dates=['GMTInterval'], \n",
    "                   infer_datetime_format = True\n",
    "                  )\n",
    "\n",
    "    # fix this one error - end was left off of this table's timestamp\n",
    "    df.rename(columns={'GMTInterval':'GMTIntervalEnd'}, inplace=True)\n",
    "\n",
    "    print (df.columns.values)\n",
    "    print (df)\n",
    "        \n",
    "#   in this source file, GMTIntervalEnd does not have a timezone.  Add it: \n",
    "    from datetime import timezone\n",
    "    df['GMTIntervalEnd'] = df['GMTIntervalEnd'].dt.tz_localize(timezone.utc)\n",
    "        \n",
    "    standardize_columns(df)  \n",
    "# interval is now redundant \n",
    "    df.drop(axis='columns', columns=['interval'], inplace=True)\n",
    "\n",
    "    con.commit();\n",
    "    \n",
    "    try:\n",
    "        con.execute(text(\"\"\"delete from stlf_vs_actual where actual is null\"\"\")); \n",
    "        con.commit(); \n",
    "    except: \n",
    "        con.rollback();\n",
    "\n",
    "    pg_insertnew(table_name=table_name, primary_keys=primary_keys, df=df, con=con)\n",
    "    \n",
    "    con.commit()\n",
    "        \n",
    "    return pgsqldf(f\"select * from {table_name} order by gmtinterval_end desc limit 5\")\n",
    "\n",
    "\n",
    "#update_stlf(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046c525",
   "metadata": {},
   "source": [
    "# MTLF Vs. Actual\n",
    "    https://marketplace.spp.org/file-browser-api/download/mtlf-vs-actual?path=%2F2023%2F02%2F25%2FOP-MTLF-202302251600.csv\n",
    "    \n",
    "## Todo:  \n",
    "DONE same thing as STLF re null values\n",
    "- this is kind of big and doesn't update but once an hour; maybe cache the file?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mtlf(con):\n",
    "    \n",
    "    # Pull out of generation_mix the most recent interval, in a format needed to get other information: \n",
    "    ci = get_current_interval()\n",
    "    \n",
    "    rt_yyyy=ci.rt_yyyy.values[0]\n",
    "    da_yyyy=ci.da_yyyy.values[0]\n",
    "    rt_mm  =ci.rt_mm.values[0]\n",
    "    da_mm  =ci.da_mm.values[0]\n",
    "    rt_dd  =ci.rt_dd.values[0]\n",
    "    da_dd  =ci.da_dd.values[0]\n",
    "    rt_hh24  =ci.rt_hh24.values[0]\n",
    "    da_hh24  =ci.da_hh24.values[0]\n",
    "    rt_mi  =ci.rt_mi.values[0]\n",
    "    \n",
    "    table_name=\"mtlf_vs_actual\"\n",
    "    source_url=f\"https://marketplace.spp.org/file-browser-api/download/mtlf-vs-actual?\" + \\\n",
    "               f\"path=%2F{rt_yyyy}%2F{rt_mm}%2F{rt_dd}%2FOP-MTLF-{rt_yyyy}{rt_mm}{rt_dd}{rt_hh24}00.csv\"\n",
    "    primary_keys=['gmtinterval_end']\n",
    "    \n",
    "    \n",
    "    # if this interval exists already in the database, don't do this update\n",
    "    try: \n",
    "        test_df=pgsqldf(f\"\"\"\n",
    "        select *\n",
    "        from mtlf_vs_actual\n",
    "        where gmtinterval_end = '{da_yyyy}-{da_mm}-{da_dd} {da_hh24}:00:00'\n",
    "        and averaged_actual is NOT NULL\n",
    "        limit 5\n",
    "        \"\"\")\n",
    "\n",
    "        assert len(test_df.index) > 0\n",
    "\n",
    "        print (f\"update_mltf: found '{da_yyyy}-{da_mm}-{da_dd} {da_hh24}:00:00' already in database\")\n",
    "        return test_df\n",
    "            \n",
    "    except: \n",
    "        print (f\"update_mltf: gmtinterval_end '{da_yyyy}-{da_mm}-{da_dd} {da_hh24}:00:00' not yet in database\")\n",
    "        get_rtbm=True\n",
    "    \n",
    "    \n",
    "    print (\"reading\", source_url)\n",
    "    \n",
    "    # this file is not huge but consider caching it locally instead of reading from remote 12 times an hour\n",
    "    # or, better, test the database to see if we need to update it.\n",
    "    \n",
    "    df=pd.read_csv(source_url, \n",
    "                   parse_dates=['GMTIntervalEnd'], \n",
    "                   infer_datetime_format = True\n",
    "                  )\n",
    "        \n",
    "    # df.rename(columns={'Interval':'interval', 'GMTIntervalEnd':'gmt_interval_end', 'MTLF':'mtlf', 'Averaged Actual':'averaged_actual'}, inplace=True)\n",
    "    \n",
    "    #   in this source file, GMTIntervalEnd does not have a timezone.  Add it: \n",
    "    from datetime import timezone\n",
    "    df['GMTIntervalEnd'] = df['GMTIntervalEnd'].dt.tz_localize(timezone.utc)\n",
    "    \n",
    "    standardize_columns(df)  \n",
    "\n",
    "    # interval is now redundant \n",
    "    df.drop(axis='columns', columns=['interval'], inplace=True)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        con.execute(text(\"\"\"delete from mtlf_vs_actual where averaged_actual is null\"\"\")); \n",
    "        con.commit(); \n",
    "    except: \n",
    "        con.rollback();\n",
    "        \n",
    "\n",
    "    pg_insertnew(table_name=table_name, primary_keys=primary_keys, df=df, con=con)\n",
    "    \n",
    "    con.commit()\n",
    "        \n",
    "    return pgsqldf(f\"select * from {table_name} where averaged_actual is not null order by gmtinterval_end desc limit 5\")\n",
    "\n",
    "#update_mtlf(con)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b14d1e",
   "metadata": {},
   "source": [
    "# Tie Flows\n",
    "    ftp://pubftp.spp.org/Operational_Data/TIE_FLOW/TieFlows.csv\n",
    "    \n",
    "## TODO: \n",
    "- Implement periodic vacuum from all these deletes\n",
    "\n",
    "- Completely refactor to convert wide-form data to long-form data.  That would avoid completely breaking this interface if an area is added, removed or renamed.\n",
    "    * AND it would completely fix the delete problem by not inserting NULL values in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED fragile wide-form; if the layout changes, we have to redo everything.  See now update_tie_flows_long. \n",
    "def update_tie_flows(con):\n",
    "    return; \n",
    "\n",
    "    table_name=\"tie_flows\"\n",
    "    source_url=\"ftp://pubftp.spp.org/Operational_Data/TIE_FLOW/TieFlows.csv\"\n",
    "    primary_keys=['gmttime']\n",
    "    \n",
    "    df=pd.read_csv(source_url, \n",
    "                   parse_dates=['GMTTime'], \n",
    "                   infer_datetime_format = True\n",
    "                  )\n",
    "    \n",
    "    #df.rename(columns={'GMTTime':'gmt_time'}, inplace=True)\n",
    "    standardize_columns(df)\n",
    "    \n",
    "    try:\n",
    "        con.execute(text(\"\"\"delete from tie_flows where spp_nsi is null\"\"\"))\n",
    "        con.commit() \n",
    "    except: \n",
    "        con.rollback()\n",
    "        \n",
    "    pg_insertnew(table_name=table_name, primary_keys=primary_keys, df=df, con=con)\n",
    "    \n",
    "    con.commit()\n",
    "    \n",
    "    return pgsqldf(f\"\"\"select * from {table_name} where spp_nsi is not null order by gmttime desc limit 5\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.rollback()\n",
    "\n",
    "def update_tie_flows_long(con):\n",
    "    table_name=\"tie_flows_long\"\n",
    "    source_url=\"ftp://pubftp.spp.org/Operational_Data/TIE_FLOW/TieFlows.csv\"\n",
    "    primary_keys=['gmttime', 'area']\n",
    "    \n",
    "    df=pd.read_csv(source_url, \n",
    "                   parse_dates=['GMTTime'], \n",
    "                   infer_datetime_format = True\n",
    "                  )\n",
    "    \n",
    "    df = pd.melt(df, id_vars=['GMTTime'], ignore_index=True).dropna()\n",
    "    \n",
    "    df.rename(columns={'GMTTime':'gmttime', 'variable':'area', 'value':'mw'}, inplace=True)\n",
    "    \n",
    "    standardize_columns(df)  # also adds inserted_time\n",
    "    \n",
    "    # remove future values that will be replaced\n",
    "    #try:\n",
    "    con.execute(text(f\"\"\"\n",
    "      delete from \"{table_name}\" where area = 'SPP NSI Future'\n",
    "      and gmttime > current_timestamp\n",
    "      \"\"\"))\n",
    "    con.commit() \n",
    "    #except: \n",
    "    #    con.rollback()\n",
    "    \n",
    "    pg_insertnew(table_name=table_name, primary_keys=primary_keys, df=df, con=con)\n",
    "    \n",
    "    con.commit()\n",
    "    \n",
    "    return pgsqldf(f\"\"\"select * from {table_name} \n",
    "    where gmttime between current_timestamp - interval '2 minutes' and current_timestamp + interval '2 minutes'\n",
    "    order by random() limit 5\"\"\")\n",
    "\n",
    "\n",
    "#df = update_tie_flows_long(con)\n",
    "#con.commit()\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458da68d",
   "metadata": {},
   "source": [
    "# Real-Time Binding Constraints\n",
    "    https://marketplace.spp.org/file-browser-api/download/rtbm-binding-constraints?path=%2FRTBM-BC-latestInterval.csv\n",
    "    \n",
    " ## TODO\n",
    " DONE: figure out if we are ON or OFF the renaming of columns bandwagon - we are currently ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ffbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rt_binding(con):\n",
    "    table_name=\"rtbm_binding_constraints\"\n",
    "    source_url=\"https://marketplace.spp.org/file-browser-api/download/rtbm-binding-constraints?path=%2FRTBM-BC-latestInterval.csv\"\n",
    "    primary_keys=['gmtinterval_end', 'constraint_name']\n",
    "    \n",
    "    df=pd.read_csv(source_url, \n",
    "                   parse_dates=['GMTIntervalEnd'], \n",
    "                   infer_datetime_format = True\n",
    "                  )\n",
    "    \n",
    "    #print (df[['GMTIntervalEnd','Constraint Name','Constraint Type','NERCID','Monitored Facility']])\n",
    "\n",
    "    #   in this source file, GMTIntervalEnd does not have a timezone.  Add it: \n",
    "    from datetime import timezone\n",
    "    df['GMTIntervalEnd'] = df['GMTIntervalEnd'].dt.tz_localize(timezone.utc)\n",
    "\n",
    "    # RT binding constraints file has dupes; fix it here\n",
    "    df.drop_duplicates(subset=None, keep='first', inplace=True, ignore_index=False)\n",
    "    \n",
    "    \"\"\" df.rename(columns={'Interval':'interval', 'GMTIntervalEnd':'gmt_interval_end', \n",
    "                       'Constraint Name':'constraint_name', 'Constraint Type':'constraint_type',\n",
    "                       'NERCID':'nercid', 'TLR Level':'tlr_level', 'State':'state', 'Shadow Price':'shadow_price',\n",
    "                       'Monitored Facility':'monitored_facility', 'Contingent Facility':'contingent_facility'}, inplace=True)\n",
    "    \"\"\"\n",
    "    standardize_columns(df)\n",
    "    \n",
    "    # interval is now redundant \n",
    "    df.drop(axis='columns', columns=['interval'], inplace=True)\n",
    "\n",
    "#    con.execute(text(\"\"\"delete from rtbm_binding_constraints where \"SPP NSI\" is null\"\"\")); \n",
    "#    con.commit(); \n",
    "      \n",
    "    pg_insertnew(table_name=table_name, primary_keys=primary_keys, df=df, con=con)\n",
    "    \n",
    "    con.commit()\n",
    "    \n",
    "    return pgsqldf(f\"\"\"select * from {table_name} order by gmtinterval_end desc limit 5\"\"\")\n",
    "\n",
    "#update_rt_binding(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0834b56",
   "metadata": {},
   "source": [
    "# DONE.  \n",
    "### Below here is just calling it again to make sure that works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    update_generation_mix(con)\n",
    "    update_ace(con)\n",
    "    update_rtbm_lmp(con)\n",
    "    update_da_lmp(con)\n",
    "    update_stlf(con)\n",
    "    update_mtlf(con)\n",
    "    update_tie_flows_long(con)\n",
    "    update_rt_binding(con)\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649519a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "while False:\n",
    "    sleep(300)\n",
    "    try: \n",
    "        update_generation_mix(con)\n",
    "        update_ace(con)\n",
    "        update_rtbm_lmp(con)\n",
    "        update_da_lmp(con)\n",
    "        update_stlf(con)\n",
    "        update_mtlf(con)\n",
    "        update_tie_flows(con)\n",
    "        update_rt_binding(con)\n",
    "        con.commit()    \n",
    "    except: \n",
    "        con.rollback()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1dc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
